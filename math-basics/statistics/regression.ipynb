{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "本文主要参考：[Ridge Regression: Simple Definition](https://www.statisticshowto.com/ridge-regression/),[Parsimonious Model: Definition, Ways to Compare Models](https://www.statisticshowto.com/parsimonious-model/),[Unbiased in Statistics: Definition and Examples](https://www.statisticshowto.com/unbiased/#UE),[Shrinkage Estimator: Definition, Examples](https://www.statisticshowto.com/shrinkage-estimator/),[Regularization: Simple Definition, L1 & L2 Penalties](https://www.statisticshowto.com/regularization/)等资料，简单了解Ridge Regression的内容。\n",
    "\n",
    "Ridge Regression中文名就是很有名的“岭回归”。岭回归是一种parsimonious-model，简而言之就是一种在预测因子 predictor variables（自变量）个数比较多，多过观测数的时候，或者预测因子之间有相关性的时候（multicollinearity）用的一种回归方法。\n",
    "\n",
    "和最熟悉的最小二乘法比较下，最小二乘法不区分预测因子的重要或不重要，所以它要包含所有预测因子，这可能导致过拟合。最小二乘法在处理 multicollinearity 时也有问题。而岭回归能够避免这些问题，部分是因为岭回归不需要无偏估计，最小二乘无偏估计方差会比较大，岭回归会增加足够的偏差来使估计更合理。\n",
    "\n",
    "简单补充下unbiased估计。\n",
    "\n",
    "统计中的unbiased是什么意思呢？在统计学上的意思是说，如果对总体参数的估计既不高估也不低估，就是unbiased。\n",
    "\n",
    "那么怎么保证样本对总体无偏呢？有多种方法，比如采样时候注意代表性等。\n",
    "\n",
    "那么什么是无偏估计？无偏估计是对总体参数的统计意义上准确的估计，准确就是说不高估也不低估，即$\\widehat \\theta = \\theta$\n",
    "\n",
    "更准确一些的定义：对于观测$X=(X_1,X_2,\\cdots,X_n)$，基于某个分布有参数$\\Theta$，d(X)是h($\\Theta$)的估计，bias就是d(X)-h($\\Theta$)。\n",
    "\n",
    "如何获得无偏估计？Minimum Variance Unbiased Estimator(MVUE)，采样时候多个样本之间方差最小。\n",
    "\n",
    "回到岭回归，岭回归使用了shrinkage estimator的一种--ridge estimator。 Shrinkage estimators 理论上产生新的估计，这些估计被收缩到离真值更近。岭估计在预测因子有multicollinearity的时候提升最小二乘法的效果不小。\n",
    "\n",
    "其中，shrinkage 是指样本中的extreme 值被收缩到cental值附近，比如样本均值，收缩的数据：\n",
    "\n",
    "- 可以获取更好，更稳定的对总体参数的估计；\n",
    "- 减少采样和非采样误差；\n",
    "- 平滑空间波动\n",
    "\n",
    "但也有一些缺点，比如Shrinkage Estimator会变有偏估计，因为低估了总体参数。\n",
    "\n",
    "Shrinkage Estimator是收缩原始估计的一种估计，比如两个extreme值可以获取一个更居中的均值。\n",
    "\n",
    "Ridge regression 使用了ridge estimator这种Shrinkage Estimator方法，产生的估计能更像真值参数收缩，ridge estimator在预测因子之间有multicollinearity时，对最小二乘法的改进效果尤其好。\n",
    "\n",
    "Ridge regression 属于使用L2 正则化的回归方法。L2正则化就是指增加L2惩罚项，所有系数使用相同的因子收缩，tuning 参数$\\lambda$来控制惩罚项强度,$\\lambda=0$则是最小二乘法，为无穷大时表示所有系数收缩到0.\n",
    "\n",
    "最小二乘法的数学表达，即系数计算公式：\n",
    "$$\\hat{B}=(X^TX)^{-1}X^TY$$\n",
    "\n",
    "岭回归增加了一个ridge参数，如下式所示，之所以称作岭回归，就是因为增加的单位矩阵的倍数会将主对角线上值变大。\n",
    "$$\\tilde{B}=(X^TX+kI)^{-1}X^TY$$\n",
    "选择k值并不是件容易的事，这也是岭回归没那么常用的一个原因。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
